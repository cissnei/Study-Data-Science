{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression으로 풀 수 있을까"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.802505 [[-0.650608   -0.92386967]]\n",
      "1000 0.696653 [[-0.05479828 -0.20491447]]\n",
      "2000 0.693323 [[ 0.02027203 -0.06013237]]\n",
      "3000 0.693178 [[ 0.01846425 -0.02456744]]\n",
      "4000 0.693156 [[ 0.01104733 -0.01198169]]\n",
      "5000 0.69315 [[ 0.00609059 -0.0062336 ]]\n",
      "[array([[ 0.5       ,  0.49844155,  0.50152266,  0.49996424]], dtype=float32), array([[ 1.,  0.,  1.,  0.]], dtype=float32), array([[False, False,  True,  True]], dtype=bool), 0.5]\n",
      "Accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "xy = np.loadtxt('train.txt', unpack=True)\n",
    "x_data = xy[0:-1]\n",
    "y_data = xy[-1]\n",
    "\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "W = tf.Variable(tf.random_uniform([1, len(x_data)], -1.0, 1.0))\n",
    "\n",
    "# Our hypothesis\n",
    "h = tf.matmul(W, X)\n",
    "hypothesis = tf.div(1., 1.+tf.exp(-h))\n",
    "\n",
    "# Cross entropy cost function\n",
    "cost = -tf.reduce_mean(Y*tf.log(hypothesis) + (1-Y)*tf.log(1-hypothesis))\n",
    "\n",
    "learning_rate = tf.Variable(0.01)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for step in range(5001):\n",
    "        sess.run(train, feed_dict={X:x_data, Y:y_data })\n",
    "        if step % 500 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={X:x_data, Y:y_data}), sess.run(W))\n",
    "    \n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.floor(hypothesis+0.5), Y)\n",
    "    # Calculate Accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(sess.run([hypothesis, tf.floor(hypothesis+0.5), correct_prediction, accuracy],\n",
    "                    feed_dict={X:x_data, Y:y_data}))\n",
    "    print(\"Accuracy:\", accuracy.eval({X:x_data, Y:y_data}))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 뭔가 이상하다!\n",
    "\n",
    "로지스틱 회귀의 선형적인 모델로는 XOR 문제를 절대 풀 수 없다는 것을 알게 되었다.  \n",
    "그렇다면 신경망으로 문제를 풀면 어떻게 될까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.752529\n",
      "500 0.690919\n",
      "1000 0.686872\n",
      "1500 0.673819\n",
      "2000 0.637046\n",
      "2500 0.581493\n",
      "3000 0.529076\n",
      "3500 0.419784\n",
      "4000 0.222037\n",
      "4500 0.122611\n",
      "5000 0.0805631\n",
      "5500 0.0590243\n",
      "6000 0.0462274\n",
      "6500 0.0378348\n",
      "7000 0.0319403\n",
      "[array([[ 0.03706356],\n",
      "       [ 0.97369903],\n",
      "       [ 0.9692477 ],\n",
      "       [ 0.03159527]], dtype=float32)]\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "xy = np.loadtxt('train.txt', unpack=True)\n",
    "x_data = np.transpose(xy[0:-1])\n",
    "y_data = np.reshape(xy[-1],(4,1))\n",
    "\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "# define 2 layer Neural Network\n",
    "W1 = tf.Variable(tf.random_uniform([2, 2], -1.0, 1.0))\n",
    "W2 = tf.Variable(tf.random_uniform([2, 1], -1.0, 1.0))\n",
    "\n",
    "b1 = tf.Variable(tf.zeros([2]), name=\"Bias1\")\n",
    "b2 = tf.Variable(tf.zeros([1]), name=\"Bias2\")\n",
    "\n",
    "# Our hypothesis\n",
    "L2 = tf.sigmoid(tf.matmul(X, W1)+b1)\n",
    "hypothesis = tf.sigmoid(tf.matmul(L2, W2)+b2)\n",
    "\n",
    "# Cross entropy cost function\n",
    "cost = -tf.reduce_mean(Y*tf.log(hypothesis) + (1-Y)*tf.log(1-hypothesis))\n",
    "\n",
    "# For this time, learning rate is very important.\n",
    "learning_rate = tf.Variable(0.1)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for step in range(7001):\n",
    "        sess.run(train, feed_dict={X:x_data, Y:y_data})\n",
    "        if step % 500 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={X:x_data, Y:y_data}))\n",
    "    \n",
    "    # Test model\n",
    "    correct = tf.equal(tf.floor(hypothesis+0.5), Y)\n",
    "    # Calculate Accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, \"float\"))\n",
    "    print(sess.run([hypothesis],feed_dict={X:x_data, Y:y_data}))\n",
    "    print(\"Accuracy:\", accuracy.eval({X:x_data, Y:y_data}))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wide Neural Network with 2 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.703788\n",
      "2000 0.604309\n",
      "4000 0.116693\n",
      "6000 0.0321776\n",
      "8000 0.016299\n",
      "10000 0.0104688\n",
      "12000 0.00756811\n",
      "14000 0.00586587\n",
      "16000 0.0047586\n",
      "18000 0.00398611\n",
      "20000 0.0034191\n",
      "22000 0.00298666\n",
      "24000 0.00264682\n",
      "26000 0.00237325\n",
      "28000 0.00214859\n",
      "30000 0.00196104\n",
      "[array([[ 0.00138587],\n",
      "       [ 0.99789822],\n",
      "       [ 0.99809533],\n",
      "       [ 0.00244387]], dtype=float32)]\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "xy = np.loadtxt('train.txt', unpack=True)\n",
    "x_data = np.transpose(xy[0:-1])\n",
    "y_data = np.reshape(xy[-1],(4,1))\n",
    "\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "# define 2 layer Neural Network - we will use 10 units\n",
    "W1 = tf.Variable(tf.random_uniform([2, 10], -1.0, 1.0))\n",
    "W2 = tf.Variable(tf.random_uniform([10, 1], -1.0, 1.0))\n",
    "\n",
    "b1 = tf.Variable(tf.zeros([10]), name=\"Bias1\")\n",
    "b2 = tf.Variable(tf.zeros([1]), name=\"Bias2\")\n",
    "\n",
    "# Our hypothesis\n",
    "L2 = tf.sigmoid(tf.matmul(X, W1)+b1)\n",
    "hypothesis = tf.sigmoid(tf.matmul(L2, W2)+b2)\n",
    "\n",
    "# Cross entropy cost function\n",
    "cost = -tf.reduce_mean(Y*tf.log(hypothesis) + (1-Y)*tf.log(1-hypothesis))\n",
    "\n",
    "# For this time, learning rate is very important.\n",
    "learning_rate = tf.Variable(0.1)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for step in range(30001):\n",
    "        sess.run(train, feed_dict={X:x_data, Y:y_data})\n",
    "        if step % 2000 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={X:x_data, Y:y_data}))\n",
    "    \n",
    "    # Test model\n",
    "    correct = tf.equal(tf.floor(hypothesis+0.5), Y)\n",
    "    # Calculate Accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, \"float\"))\n",
    "    print(sess.run([hypothesis],feed_dict={X:x_data, Y:y_data}))\n",
    "    print(\"Accuracy:\", accuracy.eval({X:x_data, Y:y_data}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 넓은 네트워크의 장점?\n",
    "\n",
    "기본 크기의 네트워크에서는 learning rate 0.1 기준 7000 epoch에서야 Accuracy 1.0에 도달했지만 넓은 신경망에서는 2000번 안에 Accuracy 1.0에 도달했다.\n",
    "\n",
    "## Deep Neural Network for XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.80091\n",
      "2000 0.689344\n",
      "4000 0.570757\n",
      "6000 0.0245225\n",
      "8000 0.00782288\n",
      "10000 0.00440981\n",
      "12000 0.00301607\n",
      "14000 0.00227221\n",
      "16000 0.00181371\n",
      "18000 0.00150441\n",
      "20000 0.00128241\n",
      "22000 0.00111577\n",
      "24000 0.000986241\n",
      "26000 0.000882844\n",
      "28000 0.000798443\n",
      "30000 0.000728336\n",
      "[array([[  4.67570469e-04],\n",
      "       [  9.99323130e-01],\n",
      "       [  9.99140620e-01],\n",
      "       [  9.08377231e-04]], dtype=float32)]\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "xy = np.loadtxt('train.txt', unpack=True)\n",
    "x_data = np.transpose(xy[0:-1])\n",
    "y_data = np.reshape(xy[-1],(4,1))\n",
    "\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "# define 2 layer Neural Network - we will use 3 layers.\n",
    "W1 = tf.Variable(tf.random_uniform([2,10], -1.0, 1.0))\n",
    "W2 = tf.Variable(tf.random_uniform([10,5], -1.0, 1.0))\n",
    "W3 = tf.Variable(tf.random_uniform([5,1], -1.0, 1.0))\n",
    "\n",
    "b1 = tf.Variable(tf.zeros([10]), name=\"Bias1\")\n",
    "b2 = tf.Variable(tf.zeros([5]), name=\"Bias2\")\n",
    "b3 = tf.Variable(tf.zeros([1]), name=\"Bias3\")\n",
    "\n",
    "# Our hypothesis\n",
    "L2 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "L3 = tf.sigmoid(tf.matmul(L2, W2) + b2)\n",
    "hypothesis = tf.sigmoid(tf.matmul(L3, W3) + b3)\n",
    "\n",
    "# Cross entropy cost function\n",
    "cost = -tf.reduce_mean(Y*tf.log(hypothesis) + (1-Y)*tf.log(1-hypothesis))\n",
    "\n",
    "# For this time, learning rate is very important.\n",
    "learning_rate = tf.Variable(0.1)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for step in range(30001):\n",
    "        sess.run(train, feed_dict={X:x_data, Y:y_data})\n",
    "        if step % 2000 == 0:\n",
    "            print(step,\n",
    "                  sess.run(cost, feed_dict={X:x_data, Y:y_data})\n",
    "                  #sess.run(W1),\n",
    "                  #sess.run(W2)\n",
    "                 )\n",
    "    \n",
    "    # Test model\n",
    "    correct = tf.equal(tf.floor(hypothesis+0.5), Y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, \"float\"))\n",
    "    \n",
    "    print(sess.run([hypothesis],feed_dict={X:x_data, Y:y_data}))\n",
    "    print(\"Accuracy:\", accuracy.eval({X:x_data, Y:y_data}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep 하면?\n",
    "\n",
    "epoch를 적게(2000) 주었을 때에는 2 Layer Wide NN보다 못한 성과를 내었으나 step을 많이 반복하자 더 효율적이라는 걸 깨달았다.  \n",
    "\n",
    "### 고려해볼 점\n",
    "\n",
    "- Learning rate(학습률)  \n",
    "- Epoch(step)  \n",
    "- 레이어의 개수\n",
    "\n",
    "### 실험"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.696796\n",
      "2000 0.686248\n",
      "4000 0.00259796\n",
      "6000 0.000818337\n",
      "8000 0.00046907\n",
      "10000 0.000324707\n",
      "12000 0.000246691\n",
      "14000 0.000198132\n",
      "16000 0.000165134\n",
      "18000 0.000141363\n",
      "20000 0.00012339\n",
      "22000 0.000109336\n",
      "24000 9.80994e-05\n",
      "26000 8.89045e-05\n",
      "28000 8.12297e-05\n",
      "30000 7.47621e-05\n",
      "[array([[  5.11028848e-05],\n",
      "       [  9.99920249e-01],\n",
      "       [  9.99932170e-01],\n",
      "       [  1.00352103e-04]], dtype=float32)]\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "xy = np.loadtxt('train.txt', unpack=True)\n",
    "x_data = np.transpose(xy[0:-1])\n",
    "y_data = np.reshape(xy[-1],(4,1))\n",
    "\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "# define 2 layer Neural Network - we will use 3 layers.\n",
    "W1 = tf.Variable(tf.random_uniform([2,10], -1.0, 1.0))\n",
    "W2 = tf.Variable(tf.random_uniform([10,10], -1.0, 1.0))\n",
    "W3 = tf.Variable(tf.random_uniform([10,10], -1.0, 1.0))\n",
    "W4 = tf.Variable(tf.random_uniform([10,1], -1.0, 1.0))\n",
    "\n",
    "b1 = tf.Variable(tf.zeros([10]), name=\"Bias1\")\n",
    "b2 = tf.Variable(tf.zeros([10]), name=\"Bias2\")\n",
    "b3 = tf.Variable(tf.zeros([10]), name=\"Bias3\")\n",
    "b4 = tf.Variable(tf.zeros([1]), name=\"Bias4\")\n",
    "\n",
    "# Our hypothesis\n",
    "L2 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "L3 = tf.sigmoid(tf.matmul(L2, W2) + b2)\n",
    "L4 = tf.sigmoid(tf.matmul(L3, W3) + b2)\n",
    "hypothesis = tf.sigmoid(tf.matmul(L4, W4) + b4)\n",
    "\n",
    "# Cross entropy cost function\n",
    "cost = -tf.reduce_mean(Y*tf.log(hypothesis) + (1-Y)*tf.log(1-hypothesis))\n",
    "\n",
    "# For this time, learning rate is very important.\n",
    "learning_rate = tf.Variable(0.3)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for step in range(30001):\n",
    "        sess.run(train, feed_dict={X:x_data, Y:y_data})\n",
    "        if step % 2000 == 0:\n",
    "            print(step,\n",
    "                  sess.run(cost, feed_dict={X:x_data, Y:y_data})\n",
    "                  #sess.run(W1),\n",
    "                  #sess.run(W2)\n",
    "                 )\n",
    "    \n",
    "    # Test model\n",
    "    correct = tf.equal(tf.floor(hypothesis+0.5), Y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, \"float\"))\n",
    "    \n",
    "    print(sess.run([hypothesis],feed_dict={X:x_data, Y:y_data}))\n",
    "    print(\"Accuracy:\", accuracy.eval({X:x_data, Y:y_data}))\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
