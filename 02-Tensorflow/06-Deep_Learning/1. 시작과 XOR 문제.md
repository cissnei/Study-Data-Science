## 생각하는 기계?

뇌를 모방한 알고리즘을 만들 수 없을까? 뉴런은 어떻게 동작을 할까?...  
이런 고민에서 다음과 같은 이론이 생겨난다.  

x라는 신호가 들어오면 w(weight)라는 값으로 곱해지고 이러한 신호를 모두 더한 후 편향(bias)값을 더한 다음, '활성화 함수(activation function)'를 통해 계산된 값에 대해 0과 1로 출력하도록 뉴런을 구현하면 어떨까?

예를 들어서 기존에 있던 로지스틱 회귀의 가설을:

$$H(X) = \frac{1}{1+e^{-W^TX}}$$  
$$H(X) = \frac{1}{1+e^{-W^TX}}$$  
$$H(X) = \frac{1}{1+e^{-W^TX}}$$  

이런 식으로 반복한다면 어떨까.

![네트워크](/network.png)

### AND OR 문제

#### AND

0 + 0 = 0  
0 + 1 = 0  
1 + 0 = 0  
1 + 1 = 1

#### OR

0 + 0 = 0  
0 + 1 = 1  
1 + 0 = 1  
1 + 1 = 1

![andor](.andor.png)

AND OR 문제는 선형적으로 구분이 가능하다. (hyperlane?)  

### XOR

0 + 0 = 0  
1 + 1 = 0  
1 + 0 = 1  
0 + 1 = 1

![xor](.xor.png)

하지만 XOR 문제는 매우 단순하지만 선형적으로 구분할 수 없다.  

그래서 이를 해결하기 위해 인간의 뉴런을 모방한 퍼셉트론을 층층이 쌓는 구조(Multilayer perceptrons, multilayer neural nets)가 제시 됐다.

이 신경망 층을 학습시킬 방법으로 역전파(Backpropagation) 알고리즘이 제시됐으며, 또 한가지 입력을 엄청나게 많은 단계로 나눠 분석하는 Convolutional Neural Networks(합성곱 신경망)모델 또한 제시됐다.  

하지만 역전파 알고리즘의 효과가 현실의 문제를 다루기 위해 계층의 수를 늘려갈수록 희미해지는 문제가 생겼다.  
그와 동시에 SVM(Support Vector Machine), 의사결정나무와 Random Forest 등의 학습 알고리즘들이 신경망의 대체제로 대두됐다.  

이후 2000년대에 들어 심층신경망에서도 weight의 초기값을 잘 설정하면 학습할 수 있다는 것이 발견되었다. 이 심층 기계 학습이 기존의 모델보다 복잡한 문제를 푸는 데에 효율적이라는 것이 확인되었으며, 심층신경망(Deep neural network)을 대신해 Deep Learning(심층학습)라는  단어가 생겨났다.

이후 합성곱 신경망(Convolutional Neural Network)이 이미지 인식 분야에서 활발히 사용되었으며 이들 모델의 정확도가 기하급수적으로 상승했다.

신경망 알고리즘이 이토록 발전한 데에는 4가지의 이유가 있었다고 Geoffrey Hinton이 제시했다.  

- 과거의 데이터셋이 현재보다 수 천배는 작았다.  
- 컴퓨터의 성능이 예전보다 수 천, 수 백만 배 발달했다.  
- weight를 제대로 초기화할 수 있게 됐다.  
- 비선형성의 잘못된 유형을 사용했다.

## 신경망으로 XOR 문제 풀기

세 개의 weight 행렬과 3개의 bias값, 3개의 출력 -- 퍼셉트론에 각각 sigmoid 함수를 취하면 XOR 문제를 풀 수 있다.  

![Sung kim](.forward.png)

이를 하나의 신경망이라고 볼 수 있다.  
또 XOR 문제를 풀 때 두 개의 퍼셉트론이 앞에서 마지막 퍼셉트론의 입력이 되었는데 이 두 개의 퍼셉트론의 weight 값과 bias 값을 행렬로 묶어 사용할 수 있다.

출력 : $$K(x) = sigmoid(XW_1+b_1)$$  
K(X)를 입력으로 받아 값 예측 : $$Y=H(X) = sigmoid(K(X)W_2+b_2)$$

```python
K = tf.sigmoid(tf.matmul(X, W_1)+b1)
hypothesis = tf.sigmoid(tf.matmul(K,W_2)+b2)
```

그렇다면 Weight값과 bias값을 계산할 수 있을까?
